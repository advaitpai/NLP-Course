{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using topic modeling, we can discover labels, by clustering topics common in documents. \n",
    "\n",
    "### Latent Dirichlet Allocation Theory:-\n",
    "\n",
    "Dirichlet Algorithm is a probablity distribution off which LDA is based. LDA was first published as a graphical mmodel for topic discovery in 2003. \n",
    "\n",
    "Assumptions:-\n",
    "\n",
    "1. Similar topics use similar words \n",
    "2. Latent topics can then be found by searching group of words that frequently occur together\n",
    "\n",
    "We need to give a set of K-topics at the start only\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Article\n",
      "0  In the Washington of 2016, even when the polic...\n",
      "1    Donald Trump has used Twitter  â€”   his prefe...\n",
      "2    Donald Trump is unabashedly praising Russian...\n",
      "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
      "4  From photography, illustration and video, to d...\n",
      "11992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer # For preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation # For topic modeling\n",
    "\n",
    "npr = pd.read_csv('npr.csv')\n",
    "print(npr.head()) # No label column\n",
    "print(len(npr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "cv = CountVectorizer(max_df=0.9,min_df=2,stop_words='english') \n",
    "# max_df: ignore terms that appear in more than 90% of the documents\n",
    "# min_df: ignore terms that appear in less than 2 documents \n",
    "# stop_words: ignore common words like 'the', 'a', 'an'\n",
    "dtm = cv.fit_transform(npr['Article']) # Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 54777\n",
      "Random Word from vocabulary: congregation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advait/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Grab the vocabulary of word\n",
    "\n",
    "print(\"Number of words in the vocabulary:\", len(cv.get_feature_names())) # Number of words in the vocabulary\n",
    "import random\n",
    "print(\"Random Word from vocabulary:\", cv.get_feature_names()[random.randint(0, len(cv.get_feature_names()))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "new\n",
      "percent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advait/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government\n",
      "company\n",
      "million\n",
      "care\n",
      "people\n",
      "health\n",
      "said\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "# Grab the topics\n",
    "print(len(LDA.components_)) # Number of topics\n",
    "\n",
    "topic = LDA.components_[0] # The topics\n",
    "top_ten_words = topic.argsort()[-10:]\n",
    "#ARGSORT -> Index positions sorted from least to greatest\n",
    "# We neeed top 10 values (10 greatest values)\n",
    "# So we extract last 10 values of argsort using [-10:]\n",
    "\n",
    "for i in top_ten_words:\n",
    "    print(cv.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "\n",
      "\n",
      "Topic 1: \n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "\n",
      "\n",
      "Topic 2: \n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "\n",
      "\n",
      "Topic 3: \n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "\n",
      "\n",
      "Topic 4: \n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "Topic 5: \n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "\n",
      "Topic 6: \n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab the highest probability words for each topic\n",
    "\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(\"Topic {}: \".format(i))\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d022b30961834b78ff4306c29056cec93bdf74ce7abe517b039f3845ba48d0a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nlp_course': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
