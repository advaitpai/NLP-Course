{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec model creates word vectors using two approaches, namely the Continous Bag Of Words (CBOW) approach or the skip-gram approach. \n",
    "\n",
    "In CBOW - Words before and after the word to be predicted are taken and then the word to be predicted is output. So multiple input, single output. \n",
    "\n",
    "In Skip-Gram - Given an input, using the auto-encoder, it tries to find the weighted probabalities of the words that are gonna show up. This process takes longer.\n",
    "\n",
    "In spacy, each vector has 300 dimensions. \n",
    "\n",
    "Since each word has a unqiue vector assigned to it in a multi-dimensional space, we can find how similar words are using cosine similarity. Cosine similarity calculates the distance between vectors.\n",
    "\n",
    "So now you can perform mathematics like this:\n",
    "\n",
    "    new_vector = king - man + woman (intuitively we can tell the answer should be queen)\n",
    "\n",
    "This new vector then attempts to find the closest vector to it, which gives the answer queen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg') # Loading large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8963e-01, -4.0309e-01,  3.5350e-01, -4.7907e-01, -4.3311e-01,\n",
       "        2.3857e-01,  2.6962e-01,  6.4332e-02,  3.0767e-01,  1.3712e+00,\n",
       "       -3.7582e-01, -2.2713e-01, -3.5657e-01, -2.5355e-01,  1.7543e-02,\n",
       "        3.3962e-01,  7.4723e-02,  5.1226e-01, -3.9759e-01,  5.1333e-03,\n",
       "       -3.0929e-01,  4.8911e-02, -1.8610e-01, -4.1702e-01, -8.1639e-01,\n",
       "       -1.6908e-01, -2.6246e-01, -1.5983e-02,  1.2479e-01, -3.7276e-02,\n",
       "       -5.7125e-01, -1.6296e-01,  1.2376e-01, -5.5464e-02,  1.3244e-01,\n",
       "        2.7519e-02,  1.2592e-01, -3.2722e-01, -4.9165e-01, -3.5559e-01,\n",
       "       -3.0630e-01,  6.1185e-02, -1.6932e-01, -6.2405e-02,  6.5763e-01,\n",
       "       -2.7925e-01, -3.0450e-03, -2.2400e-02, -2.8015e-01, -2.1975e-01,\n",
       "       -4.3188e-01,  3.9864e-02, -2.2102e-01, -4.2693e-02,  5.2748e-02,\n",
       "        2.8726e-01,  1.2315e-01, -2.8662e-02,  7.8294e-02,  4.6754e-01,\n",
       "       -2.4589e-01, -1.1064e-01,  7.2250e-02, -9.4980e-02, -2.7548e-01,\n",
       "       -5.4097e-01,  1.2823e-01, -8.2408e-02,  3.1035e-01, -6.3394e-02,\n",
       "       -7.3755e-01, -5.4992e-01,  9.9999e-02, -2.0758e-01, -3.9674e-02,\n",
       "        2.0664e-01, -9.7557e-02, -3.7092e-01,  2.7901e-01, -6.2218e-01,\n",
       "       -1.0280e-01,  2.3271e-01,  4.3838e-01,  3.2445e-02, -2.9866e-01,\n",
       "       -7.3611e-02,  7.1594e-01,  1.4241e-01,  2.7770e-01, -3.9892e-01,\n",
       "        3.6656e-02,  1.5759e-01,  8.2014e-02, -5.7343e-01,  3.5457e-01,\n",
       "        2.2491e-01, -6.2699e-01, -8.8106e-02,  2.4361e-01,  3.8533e-01,\n",
       "       -1.4083e-01,  1.7691e-01,  7.0897e-02,  1.7951e-01, -4.5907e-01,\n",
       "       -8.2120e-01, -2.6631e-02,  6.2549e-02,  4.2415e-01, -8.9630e-02,\n",
       "       -2.4654e-01,  1.4156e-01,  4.0187e-01, -4.1232e-01,  8.4516e-02,\n",
       "       -1.0626e-01,  7.3145e-01,  1.9217e-01,  1.4240e-01,  2.8511e-01,\n",
       "       -2.9454e-01, -2.1948e-01,  9.0460e-01, -1.9098e-01, -1.0340e+00,\n",
       "       -1.5754e-01, -1.1964e-01,  4.9888e-01, -1.0624e+00, -3.2820e-01,\n",
       "       -1.1232e-02, -7.9482e-01,  3.7275e-01, -6.8710e-03, -2.5772e-01,\n",
       "       -4.7005e-01, -4.1387e-01, -6.4089e-02, -2.8033e-01, -4.0778e-02,\n",
       "       -2.4866e+00,  6.2494e-03, -1.0210e-02,  1.2752e-01,  3.4965e-01,\n",
       "       -1.2571e-01,  3.1570e-01,  4.1926e-01,  2.0056e-01, -5.5984e-01,\n",
       "       -2.2801e-01,  1.2012e-01, -2.0518e-03, -8.9764e-02, -8.0373e-02,\n",
       "        1.1969e-02, -2.6978e-01,  3.4829e-01,  7.3664e-03, -1.1137e-01,\n",
       "        6.3410e-01,  3.8449e-01, -6.2248e-01,  4.1145e-02,  2.5922e-01,\n",
       "        6.5811e-01, -4.9548e-01, -1.3030e-01, -3.8279e-01,  1.1156e-01,\n",
       "       -4.3085e-01,  3.4473e-01,  2.7109e-02, -2.5108e-01, -2.8011e-01,\n",
       "        2.1662e-01,  3.2660e-01,  5.5895e-02,  7.6077e-02, -5.2480e-02,\n",
       "        4.5928e-02, -2.5266e-01,  5.2845e-01, -1.3145e-01, -1.2453e-01,\n",
       "        4.0556e-01,  3.1877e-01,  2.4415e-02, -2.2620e-01, -6.1960e-01,\n",
       "       -4.0886e-01, -3.5534e-02, -5.5123e-03,  2.3438e-01,  8.7854e-01,\n",
       "       -2.5161e-01,  4.0600e-01, -4.4284e-01,  3.4934e-01, -5.6429e-01,\n",
       "       -2.3676e-01,  6.2199e-01, -2.8175e-01,  4.2024e-01,  1.0043e-01,\n",
       "       -1.4720e-01,  4.9593e-01, -3.5850e-01, -1.3998e-01, -2.7494e-01,\n",
       "        2.3827e-01,  5.7268e-01,  7.9025e-02,  1.7872e-02, -2.1829e-01,\n",
       "        5.5050e-02, -5.4200e-01,  1.6788e-01,  3.9065e-01,  3.0209e-01,\n",
       "        2.3040e-01, -3.9351e-02, -2.1078e-01, -2.7224e-01,  1.6907e-01,\n",
       "        5.4819e-01,  9.4888e-02,  7.9798e-01, -6.6158e-02,  1.9844e-01,\n",
       "        2.0307e-01,  4.4808e-02, -1.0240e-01, -6.9909e-02, -3.6756e-02,\n",
       "        9.5159e-02, -2.7830e-01, -1.0597e-01, -1.6276e-01, -1.8211e-01,\n",
       "       -3.1897e-01, -2.1633e-01,  1.4994e-01, -7.2057e-02,  2.2264e-01,\n",
       "       -4.5551e-01,  3.0341e-01,  1.8431e-01,  2.1681e-01, -3.1940e-01,\n",
       "        2.6426e-01,  5.8106e-01,  5.4635e-02,  6.3238e-01,  4.3169e-01,\n",
       "        9.0343e-02,  1.9494e-01,  3.5483e-01, -2.0706e-02, -7.3117e-01,\n",
       "        1.2941e-01,  1.7418e-01, -1.5065e-01,  5.3355e-02,  4.4794e-02,\n",
       "       -1.6600e-01,  2.2007e-01, -5.3970e-01, -2.4968e-01, -2.6464e-01,\n",
       "       -5.5515e-01,  5.8242e-01,  2.2295e-01,  2.4433e-01,  4.5275e-01,\n",
       "        3.4693e-01,  1.2255e-01, -3.9059e-02, -3.2749e-01, -2.7891e-01,\n",
       "        1.3766e-01,  3.8392e-01,  1.0543e-03, -1.0242e-02,  4.9205e-01,\n",
       "       -1.7922e-01,  4.1215e-02,  1.3547e-01, -2.0598e-01, -2.3194e-01,\n",
       "       -7.7701e-01, -3.8237e-01, -7.6383e-01,  1.9418e-01, -1.5441e-01,\n",
       "        8.9740e-01,  3.0626e-01,  4.0376e-01,  2.1738e-01, -3.8050e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector for a word\n",
    "nlp(u'lion').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.00720906e-01,  4.20156009e-02, -9.31793973e-02, -8.15414935e-02,\n",
       "        2.50049680e-03,  1.67080835e-01, -1.08621001e-01,  7.71560054e-03,\n",
       "        1.44465894e-01,  1.79850388e+00, -2.77828038e-01, -4.84851822e-02,\n",
       "       -7.94451088e-02, -1.16215110e-01, -1.56261414e-01,  5.96945994e-02,\n",
       "        5.02396040e-02,  1.05396795e+00, -8.51650070e-03, -2.77383685e-01,\n",
       "       -1.63225681e-01,  2.29413994e-02, -1.59576014e-02, -2.49975801e-01,\n",
       "        1.51895449e-01, -5.72511964e-02, -1.80625603e-01, -1.26084194e-01,\n",
       "        1.05212606e-01, -1.70930997e-01, -1.81344628e-01,  2.11258501e-01,\n",
       "        2.26855017e-02, -1.54004004e-02,  1.76830694e-01, -4.40463014e-02,\n",
       "        7.02560022e-02, -4.22505140e-02, -1.25777842e-02, -1.30870016e-02,\n",
       "        1.51386812e-01,  1.72551982e-02, -6.04443662e-02, -1.54603601e-01,\n",
       "        9.66587141e-02,  5.71484976e-02, -1.51370004e-01,  1.12425432e-01,\n",
       "        7.27925971e-02,  2.20516007e-02, -7.03053921e-02,  1.02219798e-01,\n",
       "       -3.19202256e-04, -2.75469432e-03, -1.68553695e-01,  5.93840070e-02,\n",
       "        5.51577024e-02,  1.55460492e-01, -1.42230988e-01, -1.04420803e-01,\n",
       "        1.97228231e-02,  8.26343894e-02,  9.71329957e-02,  1.63709402e-01,\n",
       "        5.36380894e-02, -1.33914515e-01,  4.43174019e-02,  7.18266964e-02,\n",
       "        9.98407602e-04,  9.52821001e-02, -3.18695616e-04,  3.80619755e-03,\n",
       "        1.12726606e-01,  6.03505075e-02,  4.03406024e-02, -8.86157975e-02,\n",
       "       -1.26647040e-01, -7.40706101e-02, -5.28929522e-03, -2.54434049e-02,\n",
       "       -7.53790420e-03,  1.80985600e-01, -6.54187053e-02,  2.44434979e-02,\n",
       "       -6.45967107e-03, -1.23795494e-01,  4.82588679e-01,  2.37334535e-01,\n",
       "        2.91353285e-01,  9.52918082e-02, -7.16288090e-02,  1.27085894e-01,\n",
       "       -8.14299274e-04, -1.68456107e-01, -8.80299322e-03, -2.21132971e-02,\n",
       "       -1.42945692e-01, -6.86127022e-02,  1.18983999e-01, -1.20911598e-01,\n",
       "        1.41415998e-01, -2.67890096e-02, -1.23604193e-01, -6.45518005e-02,\n",
       "        1.74775213e-01, -6.69227004e-01,  2.45518476e-01, -2.29876973e-02,\n",
       "        5.50104976e-02, -1.36496097e-01,  6.64381012e-02, -1.61096588e-01,\n",
       "        1.19297601e-01, -2.32451916e-01, -1.89309027e-02,  9.96528119e-02,\n",
       "       -3.46718058e-02, -3.09060998e-02,  2.09038090e-02,  2.43077967e-02,\n",
       "        2.00498998e-02, -1.38338417e-01, -9.55733806e-02, -1.04228660e-01,\n",
       "       -1.92356288e-01,  8.52368921e-02,  7.57701695e-04, -1.08489096e-01,\n",
       "       -3.32810287e-03, -1.01179238e-02, -1.39051884e-01, -6.37444034e-02,\n",
       "       -1.28923595e-01,  6.28546029e-02,  7.86976963e-02, -3.35438997e-02,\n",
       "       -8.34110230e-02,  3.29509005e-02,  7.88877457e-02, -2.61009000e-02,\n",
       "       -1.92775989e+00, -6.22843020e-02,  1.65002286e-01,  1.03676811e-01,\n",
       "        1.62839696e-01, -9.56221968e-02,  1.21853076e-01,  1.11579942e-02,\n",
       "        1.35747582e-01,  5.12418374e-02,  3.28354128e-02, -1.13470014e-02,\n",
       "       -7.92660285e-03,  1.44048985e-02, -2.07078815e-01,  4.32069879e-03,\n",
       "       -1.20010197e-01,  1.25090703e-01, -1.21690030e-03, -2.03544423e-01,\n",
       "       -6.46209717e-03, -5.92495091e-02, -1.99140683e-01, -5.09750023e-02,\n",
       "       -4.40611020e-02, -1.11934818e-01, -1.72570087e-02, -1.35663003e-02,\n",
       "       -2.75655389e-02, -6.04875013e-02,  1.08021602e-01, -1.26475003e-02,\n",
       "       -1.80688888e-01, -1.38053983e-01, -2.27199227e-01,  3.62095051e-02,\n",
       "        1.17343917e-01,  5.05640022e-02, -2.18146034e-02,  1.60839409e-01,\n",
       "       -1.63384024e-02, -1.75512403e-01, -1.86540008e-01, -1.72549099e-01,\n",
       "        1.71970986e-02,  1.09915398e-01,  6.09103031e-02, -9.17505026e-02,\n",
       "        1.07646391e-01, -9.73550826e-02, -1.63324028e-02,  1.88549999e-02,\n",
       "        9.77551118e-02, -6.88658953e-02,  5.21255955e-02,  2.07277387e-01,\n",
       "        1.62212998e-01, -1.37234926e-02,  7.60828983e-03,  5.80336936e-02,\n",
       "       -2.36018971e-01,  4.37765978e-02, -2.38406017e-01,  6.99894056e-02,\n",
       "        1.54425204e-01,  9.56545025e-02,  6.08335026e-02, -7.87649006e-02,\n",
       "        1.10263349e-02, -3.45872305e-02, -3.60360066e-03,  1.14869699e-01,\n",
       "       -6.72789961e-02,  8.20214003e-02,  5.43986075e-02,  9.85429958e-02,\n",
       "       -1.65949598e-01,  1.59863189e-01, -8.66185874e-02,  2.22350195e-01,\n",
       "        1.13262191e-01,  7.86379129e-02, -3.01940925e-02, -1.38508473e-02,\n",
       "       -1.42477870e-01, -5.83897009e-02,  5.95925860e-02,  1.37318701e-01,\n",
       "        8.19023103e-02,  2.20127419e-01, -1.54250413e-01,  1.19520130e-03,\n",
       "        6.18612692e-02,  7.05655962e-02,  6.65967017e-02, -2.57950984e-02,\n",
       "       -5.40390471e-03, -7.82744437e-02,  1.01455212e-01,  1.04246795e-01,\n",
       "       -1.07219994e-01, -7.25165009e-02, -1.14067197e-01,  5.74019691e-03,\n",
       "        7.89737678e-04, -1.44046903e-01, -1.12714604e-01, -2.67959945e-02,\n",
       "       -1.63664714e-01,  1.01660989e-01,  2.43246034e-02, -5.68908975e-02,\n",
       "       -2.66678520e-02,  6.55577034e-02,  8.16171616e-02,  1.87330693e-01,\n",
       "        1.41182497e-01,  4.43075001e-02, -3.73008028e-02, -6.61376938e-02,\n",
       "        1.01003587e-01,  1.09452903e-01,  1.02653109e-01, -2.08737612e-01,\n",
       "        1.47912502e-01, -2.05210522e-01, -4.75588031e-02, -6.50241077e-02,\n",
       "        1.11758098e-01,  9.33832973e-02, -3.45214941e-02,  9.22184065e-02,\n",
       "       -3.87187712e-02, -1.57010302e-01, -2.79342569e-02, -2.83885896e-02,\n",
       "        5.88397905e-02,  9.26236957e-02, -4.04470414e-03,  1.66165121e-02,\n",
       "        1.76728323e-01,  2.10406072e-02, -7.06340000e-02,  7.10787922e-02,\n",
       "        8.72296020e-02, -3.22782025e-02,  4.55909930e-02, -5.51399309e-03,\n",
       "        1.26500100e-01, -5.22068068e-02, -1.35751605e-01,  6.65428936e-02,\n",
       "       -1.04643896e-01, -2.00296327e-01, -1.11768797e-01,  9.22017992e-02,\n",
       "        1.28805503e-01,  9.10602063e-02, -1.30055904e-01, -9.67930444e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vector for a sentence\n",
    "nlp(u'The quick brown fox jumps over the lazy dog.').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "#The shape of both the above vectors remain the same since the vector of the sentence is the average of all vectors in the \n",
    "print(nlp(u'lion').vector.shape)\n",
    "print(nlp(u'The quick brown fox jumps over the lazy dog.').vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.5265438\n",
      "lion pet 0.39923766\n",
      "cat lion 0.5265438\n",
      "cat cat 1.0\n",
      "cat pet 0.7505457\n",
      "pet lion 0.39923766\n",
      "pet cat 0.7505457\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "#Similarity between tokens\n",
    "tokens = nlp(u'lion cat pet')\n",
    "for t in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(t.text,t2.text,t.similarity(t2))\n",
    "\n",
    "#Simlarity values are between 0 and 1\n",
    "#Similarity is Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.657904\n",
      "like hate 0.65746516\n",
      "love like 0.657904\n",
      "love love 1.0\n",
      "love hate 0.63930994\n",
      "hate like 0.65746516\n",
      "hate love 0.63930994\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "#Similarity between tokens with opposite uses but used in similar contexts\n",
    "# e.g. is you either like, love or hate a book\n",
    "\n",
    "tokens = nlp(u'like love hate')\n",
    "for t in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(t.text,t2.text,t.similarity(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see like-love and like-hate have almost same similarity since they are often used in the same context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684830"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab.vectors) # Number of unique words for which vectors are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "nargle False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat nargle')\n",
    "for t in tokens:\n",
    "    print(t.text,t.has_vector,t.vector_norm,t.is_oov) ## is_oov checks if the word is out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'woman', 'she', 'lion', 'who', 'fox', 'brown', 'when', 'dare', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda v1, v2: 1 - spatial.distance.cosine(v1, v2)\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "new_vec = king - man + woman\n",
    "comp_similar = []\n",
    "\n",
    "for w in nlp.vocab: \n",
    "    if w.has_vector:\n",
    "        if w.is_lower:\n",
    "            if w.is_alpha:\n",
    "                similarity = cosine_similarity(new_vec,w.vector)\n",
    "                comp_similar.append((w,similarity))\n",
    "\n",
    "comp_similar = sorted(comp_similar,key = lambda item:-item[1])\n",
    "print ([t[0].text for t in comp_similar[:10]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d022b30961834b78ff4306c29056cec93bdf74ce7abe517b039f3845ba48d0a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nlp_course': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
